# Задача
Есть большое количество файлов в формате .npy, которые разлиты на различные части. 

Необходимо преобразовать все эти данные в csv формат и загрузить на hugging face, с возможностью дальнейшего восстановления исходных данных и корректного раскладывания их по папкам:
http://olmo-data.org/preprocessed/proof-pile-2/v0_decontaminated/algebraic-stack/train/allenai/dolma2-tokenizer/part-00-00000.npy должен лежать в папке proof-pile-2/v0_decontaminated/algebraic-stack/train/allenai/dolma2-tokenizer/ и называться part-00-00000.npy
Кроме того данные будут переправляться по сети, поэтому необходимо минимизировать объем передаваемых данных.
Каждый part весит достаточно много: 16 гигабайт, поэтому желательно уметь сохранять и восстанавливаться из памяти для ускорения процесса загрузки при проблемах с сетью. Также после загрузки данных на hugging face, необходимо удалять исходные файлы.

# Решение
Код состоит из двух утилит: `src/data_processor.py` (конвертация и выгрузка) и `src/data_reconstructor.py` (восстановление исходной структуры). Общие функции вынесены в `src/utils.py`.

- Загружаем `.npy` по URL потоково, считаем SHA256, извлекаем относительный путь датасета из URL (между `preprocessed/` и `part-`), выделяем индекс части `part-*.npy`.
- Конвертируем в CSV c двумя колонками: `value` (uint32) и `part` (строка, хранит полный `part-*.npy`). Для очень больших частей пишем по слайсам, используя memory‑map и лимиты на файл: до 2e9 строк или ~15 ГБ. При переполнении создаём следующий файл с суффиксом `_chunk_N`.
- Имя CSV кодирует исходный групповой путь (замена `/` на `__`). Прогресс сохраняется в `processed_data/processing_progress.json`: резюмируемся с последнего безопасного места и помечаем файлы обработанными только после успешной записи чанка.
- При включённой опции выгружаем каждый сохранённый CSV на Hugging Face и по желанию удаляем локальный файл после успешной загрузки. Хэши (`file_hashes.json`) также обновляются и могут быть выгружены.
- Восстановление читает один или несколько CSV одного набора (основной файл плюс `_chunk_*`), группирует по `part` и собирает массивы обратно. Файлы сохраняются как настоящие `.npy` в исходной иерархии директорий.